{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSI 5138 Assignment 3 IMDB text vectorizing",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJ/I94bwR9b7PImJXj3zZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaryDongsn/DL_RL/blob/master/CSI_5138_Assignment_3_IMDB_text_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7lRDkAz6Ouw",
        "outputId": "d82b2bea-6932-4967-ff65-821d417c73bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "batch_size = 1000\n",
        "seed = 20\n",
        "max_features = 10000\n",
        "sequence_length = 500\n",
        "embedding_dim = 32\n",
        "\n",
        "\n",
        "def getRawData():\n",
        "\n",
        "  # Download IMDB data \n",
        "  url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "  dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                      untar=True, cache_dir='.',\n",
        "                                      cache_subdir='')\n",
        "\n",
        "  #build the dataset \n",
        "  dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "  train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "  remove_dir = os.path.join(train_dir, 'unsup')\n",
        "  shutil.rmtree(remove_dir)\n",
        "\n",
        "  raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "      'aclImdb/train', \n",
        "      batch_size=batch_size, \n",
        "      validation_split=0.2, \n",
        "      subset='training', \n",
        "      seed=seed)\n",
        "\n",
        "  raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "      'aclImdb/train', \n",
        "      batch_size=batch_size, \n",
        "      validation_split=0.2, \n",
        "      subset='validation', \n",
        "      seed=seed)\n",
        "\n",
        "  raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "      'aclImdb/test', \n",
        "      batch_size=batch_size)\n",
        "\n",
        "  return raw_train_ds, raw_val_ds, raw_test_ds\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')\n",
        "  \n",
        "raw_train_ds, raw_val_ds, raw_test_ds = getRawData()\n",
        "\n",
        "def getVectorizeLayer():\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=custom_standardization,\n",
        "      max_tokens=max_features,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length\n",
        "      )\n",
        "\n",
        "  # Make a text-only dataset (without labels), then call adapt\n",
        "  train_text = raw_train_ds.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  return vectorize_layer\n",
        "vectorize_layer = getVectorizeLayer()\n",
        "\n",
        "  \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 3s 0us/step\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nf7r1d8RdyF"
      },
      "source": [
        "def getVectorizeText(text, label):\n",
        "  \n",
        "  text = tf.expand_dims(text, -1)\n",
        "  text_vectorized = vectorize_layer(text)\n",
        "  return text_vectorized, label"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWloXRk5-K9Y",
        "outputId": "86cb488c-6149-4d9f-d514-14ef635d93bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review:\", first_review.numpy())\n",
        "print(\"Label:\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", getVectorizeText(first_review, first_label))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: b'Clint Eastwood would star again as the battle-weary Detective Harry Callahan, but would also direct the fourth entry in the \\'Dirty Harry\\' series. \\'Sudden Impact\\' again like the other additions, brings its own distinguishable style and tone, but if anything it\\'s probably the most similar to the original in it\\'s darker and seedy moments (and bestowing a classic line \"Go ahead. Make my day\")\\xc2\\x85 but some of its humor has to been seen to believe. A bulldog\\xc2\\x85 named meathead that pisses and farts. Oh yeah. However an interesting fact this entry was only one in series to not have it set entirely in San Francisco.<br /><br />The story follows that of detective Callahan trying to put the pieces together of a murder where the victim was shot in the groin and then between the eyes. After getting in some trouble with office superiors and causing a stir which has some crime lord thugs after his blood. He\\'s ordered to take leave, but it falls into a working one where he heads to a coastal town San Paulo, where a murder has occurred similar in vein (bullet to groin and between eyes) to his case. There he begins to dig up dirt, which leads to the idea of someone looking for revenge.<br /><br />To be honest, I wasn\\'t all that crash hot on Eastwood\\'s take, but after many repeat viewings it virtually has grown on me to the point of probably being on par with the first sequel \\'Magnum Force\\'. This well-assembled plot actually gives Eastwood another angle to work upon (even though it feels more like a sophisticated take on the vigilante features running rampant at that time), quite literal with something punishing but luridly damaging. It\\'s like he\\'s experimenting with noir-thriller touches with character-driven traits to help develop the emotionally bubbling and eventual morality framework. His use of images is lasting, due to its slickly foreboding atmospherics. Dark tones, brooding lighting\\xc2\\x85 like the scene towards the end akin to some western showdown of a silhouette figure (Harry with his new .44 automag handgun) moving its way towards the stunned prey on the fishing docks. It\\'s a striking sight that builds fear! Mixing the hauntingly cold with plain brutality and dash of humor. It seemed to come off. A major plus with these films are the dialogues, while I wouldn\\'t call \\'Sudden Impact\\' first-rate, it provides ample biting exchanges and memorably creditable lines\\xc2\\x85 \"You\\'re a legend in your own mind\". Don\\'t you just love hearing Harry sparking an amusing quip, before pulling out his piece. The beating action when it occurs is excitingly jarring and intense\\xc2\\x85 the only way to go and the pacing flies by with little in the way of flat passages. Lalo Schfrin would return as composer (after \\'The Enforcer\" had Jerry Fielding scoring) bringing a methodical funky kick, which still breathed those gloomy cues to a texturally breezy score that clicked from the get-go. Bruce Surtees (an Eastwood regular) gets the job behind the camera (where he did a piecing job with \\'Dirty Harry\\') and gives the film plenty of scope by wonderfully framing the backdrops in some impeccable tracking scenes, but also instrument edgy angles within those dramatic moments.<br /><br />Eastwood as the dinosaur Callahan still packs a punch, going beyond just that steely glare to get the job done and probably showing a little more heart than one would expect from a younger Callahan. This going by the sudden shift in a plot turn of Harry\\'s quest for justice\\xc2\\x85 by the badge even though he doesn\\'t always agree with it. I just found it odd\\xc2\\x85 a real change of heart. Across from him is a stupendous performance by his beau at the time Sondra Locke. Her turn of traumatic torment (being senselessly raped along with her younger sister), is hidden by a glassily quiet intensity. When the anger is released, it\\'s tactically accurate in its outcome. Paul Drake is perfectly menacing and filthy as one of the targeted thugs and Audrie J. Neenan nails down a repellently scummy and big-mouthed performance. These people are truly an ugly bunch of saps. Pat Hingle is sturdy as the Chief of the small coastal town. In smaller parts are Bradford Dillman and the agreeably potent Albert Popwell (a regular in the series 1-4, but under different characters). How can you forget him in \\'Dirty Harry\\'\\xc2\\x85 yes he is bank robber that\\'s at the end of the trademark quote \"Do I feel lucky? Well, do ya, punk?\"'\n",
            "Label: pos\n",
            "Vectorized review (<tf.Tensor: shape=(1, 500), dtype=int64, numpy=\n",
            "array([[4125, 4633,   59,  333,  172,   14,    2,    1, 1260, 1425,    1,\n",
            "          18,   59,   78, 1567,    2, 2743, 3025,    8,    2, 1685, 1425,\n",
            "         200, 2138, 1489,  172,   38,    2,   82,    1,  933,   29,  199,\n",
            "           1,  436,    4, 1152,   18,   44,  230,   29,  235,    2,   86,\n",
            "         729,    6,    2,  198,    8,   29, 3663,    4, 5826,  372,    4,\n",
            "           1,    3,  343,  361,  138, 1378,   99,   54,    1,   18,   46,\n",
            "           5,   29,  479,   43,    6,   74,  104,    6,  257,    3,    1,\n",
            "         767,    1,   12,    1,    4,    1,  455, 1257,  187,   33,  213,\n",
            "         184,   11, 3025,   13,   60,   28,    8,  200,    6,   21,   25,\n",
            "           9,  275, 1054,    8, 2647, 3508,    2,   62, 1151,   12,    5,\n",
            "        1260,    1,  262,    6,  269,    2, 1354,  287,    5,    3,  580,\n",
            "         113,    2, 1394,   13,  317,    8,    2,    1,    4,   94,  190,\n",
            "           2,  516,  100,  386,    8,   46, 1067,   16, 1056, 9488,    4,\n",
            "        4256,    3,    1,   61,   43,   46,  874, 1615, 3671,  100,   24,\n",
            "         536,  228, 5251,    6,  188,  557,   18,    9,  704,   77,    3,\n",
            "         802,   28,  113,   27, 1808,    6,    3,    1,  517, 2647,    1,\n",
            "         113,    3,  580,   43, 3842,  729,    8, 5574, 4486,    6,    1,\n",
            "           4,  190,  516,    6,   24,  418,   47,   27,  744,    6, 3129,\n",
            "          56, 5951,   61,  769,    6,    2,  313,    5,  282,  281,   15,\n",
            "        1055,    6,   26, 1209,   10,  271,   31,   12, 2395,  916,   20,\n",
            "           1,  188,   18,  100,  105, 3019, 4745,    9, 2330,   43, 2216,\n",
            "          20,   69,    6,    2,  212,    5,  235,  108,   20, 2951,   16,\n",
            "           2,   85,  730,    1, 1178,   11,    1,  109,  157,  390, 4633,\n",
            "         155, 2485,    6,  159,  723,   53,  150,    9,  743,   51,   38,\n",
            "           3, 3356,  188,   20,    2, 8545,  922,  617, 7327,   30,   12,\n",
            "          58,  177, 6960,   16,  137,    1,   18,    1,    1,   29,   38,\n",
            "         228,    1,   16,    1, 2504,   16,    1, 6589,    6,  330, 2068,\n",
            "           2, 2121,    1,    4, 6333, 3713, 9877,   24,  339,    5, 1201,\n",
            "           7, 6532,  672,    6,   29,    1, 8676,    1,  447, 7430, 6464,\n",
            "           1,   38,    2,  134,  905,    2,  125, 5880,    6,   46,  995,\n",
            "        4659,    5,    3,    1,  813, 1425,   16,   24,  156,    1,    1,\n",
            "           1,  740,   29,   91,  905,    2, 4597, 5300,   20,    2, 5199,\n",
            "           1,   29,    3, 3310, 1658,   12, 4092, 1073, 6300,    2,    1,\n",
            "        1076,   16, 1021, 5426,    4, 7966,    5,  479,    9,  446,    6,\n",
            "         208,  124,    3,  669,  908,   16,  128,   95,   23,    2, 4297,\n",
            "         133,   10,  552,  637, 2138, 1489, 7108,    9, 1467, 6905, 7270,\n",
            "           1,    4,    1,    1,    1,  324,    3, 1660,    8,  123,  199,\n",
            "         346,   89,   22,   41,  116, 2160, 1425,    1,   33, 1116,    1,\n",
            "         153, 3589,   45,   24,  405,    2, 3326,  214,   50,    9, 3358,\n",
            "           7,    1, 6850,    4,    1,    2,   60,   91,    6,  138,    4,\n",
            "           2, 1828, 4118,   32,   16,  111,    8,    2,   91,    5, 1063,\n",
            "           1,    1,    1,   59,  947,   14, 4789,  100,    2,    1,   66,\n",
            "        1473,    1,    1, 2290,    3,    1, 8094, 2159,   61,  127,    1,\n",
            "         144, 7221,    1,    6,    3,    1,    1,  583,   12,    1,   35,\n",
            "           2,    1, 1376,    1,   33, 4633, 1941,  204,    2,  285,  496,\n",
            "           2,  382,  113,   27,  114]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebhaw2DPQ_Xu"
      },
      "source": [
        "def getVectorizedData():\n",
        "  train_ds = raw_train_ds.map(getVectorizeText)\n",
        "  val_ds = raw_val_ds.map(getVectorizeText)\n",
        "  test_ds = raw_test_ds.map(getVectorizeText)\n",
        "  return train_ds, val_ds, test_ds\n",
        "train_ds, val_ds, test_ds = getVectorizedData ()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}